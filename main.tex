\documentclass[12pt]{report}

\usepackage{polyglossia}
\setdefaultlanguage{french}
\usepackage[french=guillemets]{csquotes}
\MakeOuterQuote{"}
\usepackage[top=2.5cm, bottom=2.5cm, left=4cm, right=3cm]{geometry}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{colonequals}
\usepackage{prftree}
\usepackage{graphicx}
\usepackage{pmboxdraw}
\usepackage{lscape}
\usepackage{longtable}
\usepackage{tabu}
\usepackage{multirow}
\usepackage{pifont}
\usepackage{cmll}
\usepackage{bbding}

\doublespacing

\prflineextra=0em
\prflinepadbefore=0.5ex
\prflinepadafter=0.5ex
\prfrulenameskip=0.5em

\tabulinesep=0.5em
\tabcolsep=0.5em
\allowdisplaybreaks

\newcommand{\lto}{\Rightarrow}
\newcommand{\lequiv}{\Leftrightarrow}
\newcommand{\dai}{\text{\CrossMaltese}}
\newcommand{\seq}{\vdash}
\newcommand{\irule}[1]{\footnotesize$#1$}
\newcommand{\iruleL}[1]{\irule{{#1}\seq}}
\newcommand{\iruleR}[1]{\irule{\seq{#1}}}

\newtheorem{definition}{Définition}

\begin{document}

\chapter*{Introduction}

La \textbf{ludique} est une théorie inventée par le mathématicien Jean-Yves Girard qui se propose de refonder la logique autour de la notion d'\emph{interaction}. Elle s'inscrit directement dans la tradition de la théorie de la démonstration, et ce de deux façons : d'abord par son origine historique et technique, qui est la découverte de la \emph{focalisation} par Andreoli \cite{And92} dans le cadre de ses recherches sur la programmation logique linéaire; puis par la place centrale accordée aux \emph{desseins}, les objets primitifs de la ludique, qui correspondent à une sorte de généralisation/abstraction des preuves formelles. La ludique a été introduite extensivement par Girard dans son article \textit{Locus Solum} \cite{Gir01}. Cet article apparaît comme l'un des premiers où l'auteur n'hésite pas à complémenter les résultats mathématiques par des réflexions philosophiques plus personnelles : on est particulièrement marqué par le positionnement très tranché sur les théories logiques concurrentes, ainsi que par le style de rédaction assez excentrique pour un article académique.\\

Outre ces quelques originalités --- que l'on retrouve jusque dans les choix terminologiques ---, la ludique a su démontrer son intérêt scientifique, à la fois du point de vue de la logique pure, et des applications à d'autres domaines tels que l'informatique et la linguistique. Dans le premier cas, on peut attribuer ce succès à deux résultats de complétude qui constituent une véritable justification technique de la pertinence de la ludique en tant que système logique\footnote{Il convient toutefois de mentionner que certains fragments de la logique usuelle n'ont pas encore trouvé de reconstruction ludique, ce qui relativise l'intérêt fondationnel de la théorie. C'est une des raisons qui ont poussé Girard à développer un cadre plus proche des mathématiques standards : la \textit{Géométrie de l'Interaction}.}. Dans le cas des applications extra-logiques, on constate une forte interaction avec l'informatique que l'on doit principalement à l'héritage de la logique linéaire : on a non seulement des travaux tels que \cite{Sau08} qui étendent le travail original d'Andreoli en programmation logique au cadre ludique, mais aussi des tentatives plus originales avec \cite{Ter11} qui se sert de la ludique pour reformuler dans un cadre unifié des résultats de théorie de la calculabilité/complexité, ou encore \cite{Fou11} qui propose de modéliser les interactions entre les différents agents du Web à l'aide de la ludique. Pour ce qui est de la linguistique, les applications actuelles sont plus particulièrement ciblés vers la \emph{pragmatique}. Les desseins mentionnés précédemment peuvent en effet être interprétés comme des stratégies argumentatives dans le cadre d'une interaction \emph{dialogique}, fournissant une approche formelle et computationnelle à l'étude du langage en tant que pratique intersubjective ancrée dans l'espace et le temps.\\

L'objectif premier de ce mémoire est de donner une présentation intégrée des motivations historiques et conceptuelles qui ont donné naissance à la ludique, en prenant pour fil rouge la problématique de la recherche automatique de preuves en calcul des séquents. On sera alors naturellement amené à envisager un système logique doué de la propriété de focalisation, qui lorsqu'elle est prise comme primitive donne lieu à la ludique. On introduira ensuite les concepts de base de la théorie abordés du point de vue des applications à la pragmatique, où l'on s'intéressera à la modélisation formelle des dialogues et de leur dynamique.

\chapter*{Origines}

Il existe aujourd'hui en théorie de la démonstration une très grande variété de systèmes de calcul des séquents. Les premiers systèmes $\mathbf{LK}$ et $\mathbf{LJ}$, conçus respectivement pour les logiques classique et intuitionniste, furent introduits par Gentzen comme outils techniques pour prouver la cohérence de l'arithmétique de manière finitiste, conformément au programme de Hilbert. Mais l'échec de ce dernier, prononcé notamment par le second théorème d'incomplétude de Gödel, incita les logiciens qui reprirent les travaux de Gentzen à étudier ces systèmes de logique pure pour eux-mêmes. Plus précisément, on commença à s'intéresser aux propriétés \emph{structurelles} des démonstrations exprimées dans ce format, amenant inévitablement à la création de variantes en tous genres possédant ou non certaines propriétés.\\

Avec l'arrivée de l'informatique, on s'intéressa de plus en plus aux propriétés \emph{computationnelles} des systèmes de calcul des séquents. En particulier, dans le paradigme du \emph{calcul comme recherche de preuves} employé en programmation logique, on cherche à optimiser le processus de recherche d'une preuve pour un séquent dans une logique donnée, afin de réduire la complexité des programmes à l'exécution. Les propriétés structurelles des démonstrations occupent dans ce cadre une place centrale, les conséquences du choix de telle ou telle règle lors de la conception d'un système allant bien au-delà de la simple préférence esthétique. L'une de ces propriétés, la \textbf{réversibilité}, se trouve être au c\oe{}ur de l'innovation technique qui a donné naissance à la ludique : la \textbf{focalisation}.

\chapter*{Le calcul des séquents}

Historiquement, le premier formalisme adopté en théorie de la démonstration afin de représenter les preuves est celui des \emph{systèmes à la Hilbert}. De nature axiomatique, ces derniers ne comprennent que très peu de règles d'inférence --- souvent uniquement le \textit{modus ponens} ---, et se prêtent donc mal à un usage pratique visant à raisonner à l'intérieur du système. De plus, cette simplicité structurelle des démonstrations formelles en fait des objets assez pauvres d'un point de vue mathématique, et donc peu aptes à concurrencer les approches sémantiques alors particulièrement prisées à l'époque. C'est très probablement ce qui a poussé Gentzen à mettre au point deux systèmes de déduction alternatifs : la \emph{déduction naturelle} dans un premier temps, puis le \emph{calcul des séquents}.\\

Les deux systèmes ont en commun d'être construits autour d'un unique axiome, déplaçant tout le contenu logique auparavant exprimé au niveau des formules directement dans les règles d'inférence, donc dans la structure même des démonstrations. Alors que la déduction naturelle tente de coller le plus possible à la façon dont le mathématicien raisonne en pratique, le calcul des séquents n'a été introduit que comme outil technique pour prouver le \textit{Haupstatz} ou \emph{théorème d'élimination des coupures}, que Gentzen n'arrivait pas à formuler en déduction naturelle, et n'apparaît donc pas de prime abord très adapté en tant qu'outil pratique pour la recherche de preuves. Or il se trouve que les preuves sans coupures, qui d'après le \textit{Haupstatz} quotientent l'ensemble des preuves sans perte de puissance déductive, possèdent des propriétés particulièrement intéressantes pour la recherche de preuves, notamment la \emph{propriété de la sous-formule} : celle-ci garantit qu'à toute étape de la déduction, les formules apparaissant dans les prémisses sont contenues dans les formules apparaissant dans la conclusion, ce qui réduit considérablement l'espace de recherche (en supposant que l'on parte de la conclusion).\\

Tout cela peut néanmoins sembler un peu abscon pour qui n'a pas déjà une bonne connaissance du calcul des séquents : on présente donc le système $\mathbf{LKp}$, qui correspond au système $\mathbf{LK}$ de Gentzen\footnote{\cite{Gen35}, pp. 190-193} réduit au cadre propositionnel, et qui sera le point de départ de notre réflexion sur la recherche de preuves. On commence par définir la notion de \emph{séquent} :

\begin{definition}
    On appelle \emph{séquent} une expression $\Gamma \seq \Delta$, où $\Gamma$ et $\Delta$ sont des suites finies de formules.
\end{definition}

Une interprétation intuitive de la signification d'un séquent consiste à lire $\Gamma \seq \Delta$ comme une \emph{implication} ayant pour antécédant la \emph{conjonction} des formules de $\Gamma$, et pour conséquent la \emph{disjonction} des formules de $\Delta$. On verra que selon le système de calcul des séquents adopté, on mettra un sens différent sur ces trois opérations logiques usuelles. En revanche, quelque soit le système, on disposera d'un unique axiome d'\textbf{identité}\footnote{On choisit de présenter les axiomes comme des règles sans prémisses, afin de les distinguer plus clairement des séquents hypothèses.}, ainsi que de la fameuse règle de \textbf{coupure} (\emph{cut} en anglais), qui correspond à la \emph{composition} des preuves en déduction naturelle :

\begin{displaymath}
    \prfbyaxiom{\irule{id}}{A \seq A}
    \qquad
    \prftree[r]{\irule{cut}}
        {\Gamma \seq A,\Delta}
        {\Gamma',A \seq \Delta'}
        {\Gamma,\Gamma' \seq \Delta,\Delta'}
\end{displaymath}

Les règles restantes sont séparées en deux groupes : les règles \textbf{structurelles} qui agissent sur la structure des séquents, et les règles \textbf{logiques} qui agissent sur la structure des formules. Chaque règle peut agir principalement soit sur la partie gauche, soit sur la partie droite du séquent conclusion. Notre convention de nommage sera d'introduire le symbole $\seq$ à la fin du nom pour les règles gauches, et au début pour les règles droites.\\

Dans $\mathbf{LKp}$, le groupe structurel est composé de trois règles dites d'\textbf{échange}, d'\textbf{affaiblissement} et de \textbf{contraction}, dénotées respectivement par les lettres $X$, $W$ et $C$ :

\begin{longtabu}{>{$}c<{$}>{$}c<{$}}
    \prftree[r]{\iruleL{X}}
        {\Gamma,A,B,\Gamma' \seq \Delta}
        {\Gamma,B,A,\Gamma' \seq \Delta} &
    \prftree[r]{\iruleR{X}}
        {\Gamma \seq \Delta,A,B,\Delta'}
        {\Gamma \seq \Delta,B,A,\Delta'} \\

    \prftree[r]{\iruleL{W}}
        {\Gamma \seq \Delta}
        {\Gamma,A \seq \Delta} &
    \prftree[r]{\iruleR{W}}
        {\Gamma \seq \Delta}
        {\Gamma \seq A,\Delta} \\

    \prftree[r]{\iruleL{C}}
        {\Gamma,A,A \seq \Delta}
        {\Gamma,A \seq \Delta} &
    \prftree[r]{\iruleR{C}}
        {\Gamma \seq A,A,\Delta}
        {\Gamma \seq A,\Delta}
\end{longtabu}

Les règles d’échange expriment simplement la possibilité de changer l’ordre des formules à gauche ou à droite d’un séquent, et seront admises dans tous les systèmes considérés dans ce mémoire. Les règles d’affaiblissement expriment la possibilité d’ajouter des formules dans un séquent, et correspondent à la \emph{décharge vide} d’hypothèses en déduction naturelle. Les règles de contraction permettent quant à elles de supprimer les occurrences redondantes des formules, et correspondent à la \emph{décharge multiple} d’hypothèses en déduction naturelle.\\

Dans le groupe logique, on dispose de règles pour introduire chaque connecteur propositionnel :

\begin{longtabu}{>{$}c<{$}>{$}c<{$}}
    \prftree[r]{\iruleL{\neg}}
        {\Gamma \seq A,\Delta}
        {\Gamma,\neg A \seq \Delta} &
    \prftree[r]{\iruleR{\neg}}
        {\Gamma,A \seq \Delta}
        {\Gamma \seq \neg A,\Delta} \\

    \prftree[r]{\iruleL{\lto}}
        {\Gamma \seq A,\Delta}
        {\Gamma',B \seq \Delta'}
        {\Gamma,\Gamma',A \lto B \seq \Delta,\Delta'} &
    \prftree[r]{\iruleR{\lto}}
        {\Gamma,A \seq B,\Delta}
        {\Gamma \seq A \lto B,\Delta} \\

    \prftree[r]{\iruleL{l\land}}
        {\Gamma,A \seq \Delta}
        {\Gamma,A \land B \seq \Delta} &
    \multirow{2}{*}{
    \prftree[r]{\iruleR{\land}}
        {\Gamma \seq A,\Delta}
        {\Gamma \seq B,\Delta}
        {\Gamma \seq A \land B,\Delta}} \\
    \prftree[r]{\iruleL{r\land}}
        {\Gamma,B \seq \Delta}
        {\Gamma,A \land B \seq \Delta} & \\

    \multirow{2}{*}{
    \prftree[r]{\iruleL{\lor}}
        {\Gamma,A \seq \Delta}
        {\Gamma,B \seq \Delta}
        {\Gamma,A \lor B \seq \Delta}} &
    \prftree[r]{\iruleR{l\lor}}
        {\Gamma \seq A,\Delta}
        {\Gamma \seq A \lor B,\Delta} \\ &
    \prftree[r]{\iruleR{r\lor}}
        {\Gamma \seq B,\Delta}
        {\Gamma \seq A \lor B,\Delta}
\end{longtabu}

\chapter*{Stratégies de recherche}

Lorsque l'on procède à la recherche d'une preuve pour un séquent donné, plusieurs stratégies sont envisageables : les premières, que l'on pourrait qualifier de \emph{top-down}\footnote{En référence au sens dans lequel s'effectue traditionnellement les dérivations.}, consistent à partir d'instances d'axiomes qui nous semblent pertinentes, puis à utiliser les règles du système adopté pour produire un certain nombre de dérivations que l'on va pouvoir composer entre elles, et ce jusqu'à arriver à la conclusion recherchée. C'est ce genre de méthode heuristique qui est utilisé en pratique en mathématiques\footnote{En remplaçant le plus souvent les axiomes par des lemmes ou des hypothèses.}, ou encore dans les systèmes à la Hilbert. Or contrairement à ces derniers, les systèmes de calcul des séquents possèdent très peu d'axiomes, et se prêtent donc assez mal à ce type de stratégie. En revanche on vient de voir qu'ils comportent beaucoup de règles de dérivation, parfois plusieurs pour un même connecteur. Cette propriété les rend particulièrement adaptés aux stratégies de recherche dites \textbf{root-first} (ou, par opposition aux premières, \emph{bottom-up}), que l'on peut résumer très succintement par l'algorithme suivant :\\

Soit $\sigma$ le séquent que l'on cherche à prouver, appelé \emph{séquent final}.
\begin{enumerate}
    \item Établir la liste $L$ des instances de règles applicables dans le système choisi qui ont $\sigma$ pour conclusion.
    \item Choisir une instance de règle $\mathsf{r}$ dans $L$.
        \begin{enumerate}
            \item Si toutes les prémisses de $\mathsf{r}$ sont des instances d'axiomes, terminer la recherche.
            \item Sinon, réitérer l'algorithme sur les séquents prémisses de $\mathsf{r}$ qui ne sont pas des instances d'axiomes.
        \end{enumerate}
\end{enumerate}
~\\
On voit donc que ces stratégies se décrivent bien de manière algorithmique, ce qui laisse penser qu'on pourrait trouver une procédure de recherche effective, c'est-à-dire qui \emph{termine}, pour notre système $\mathbf{LKp}$. On constate toutefois en pratique qu'un certain nombre d'obstacles empêchent l'existence d'une telle procédure, obstacles qui se manifestent essentiellement dès la première étape de notre algorithme : en effet, comment déterminer si une instance de règle $\mathsf{r}$ est applicable en ayant pour unique information son séquent conclusion ? Il est tout à fait possible que $\mathsf{r}$ soit valide à un niveau \emph{local}, mais inapplicable à un niveau \emph{global}, au sens où elle ne permet pas d'aboutir à une preuve du séquent final.\\

Prenons pour exemple le séquent $p \seq p \lor q$. La seule preuve de ce séquent consiste en une simple application de la règle {\iruleR{l\lor}}, qui amène à l'instance d'axiome $p \seq p$. En suivant une implémentation naïve de notre algorithme qui incluerait dans $L$ l'ensemble des instances de règles valides, on pourrait très bien opter pour une instance de la règle {\iruleR{r\lor}}, qui nous amènerait au séquent $p \seq q$ pour lequel il n'existe aucune instance de règle valide. Dans ce cas très simple, une solution un peu "brutale" consisterait à effectuer du \emph{backtracking}, c'est-à-dire revenir en arrière et essayer une autre instance de règle valide sur $p \seq p \lor q$. Néanmoins dans des cas plus complexes, le \emph{backtracking} se révèle vite impraticable du fait qu'il est très difficile de déterminer à quel séquent antérieur recommencer la recherche afin d'obtenir une preuve du séquent final; sans compter les problèmes de complexité combinatoire quand $L$ contient beaucoup d'éléments.

\chapter*{Réversibilité classique}

On voit donc que pour concevoir une procédure de recherche effective, il va être nécessaire d'obtenir plus d'informations sur la \emph{structure} des preuves dans le système considéré, afin de quotienter au maximum l'espace des preuves sans perte de pouvoir déductif. On a déjà mentionné que grâce au \emph{Haupstatz}, on peut quotienter l'ensemble des preuves de $\mathbf{LK}$ (et donc de $\mathbf{LKp}$) par l'ensemble des preuves sans coupures. Cela signifie concrètement que lors de notre procédure de recherche, il ne sera pas nécessaire d'inclure dans $L$ les instances de la règle $cut$, ce qui réduit considérablement l'espace de recherche puisque toutes les autres règles de $\mathbf{LKp}$ vérifient la \emph{propriété de la sous-formule} :

\begin{definition}
    On dit qu'une règle $\mathsf{R}$ vérifie la \emph{propriété de la sous-formule} lorsque pour toute instance $\mathsf{r}$ de $\mathsf{R}$, chaque formule contenue dans les séquents prémisses de $\mathsf{r}$ est sous-formule d'au moins une formule du séquent conclusion de $\mathsf{r}$.
\end{definition}

Néanmoins la propriété de la sous-formule ne suffit pas à éliminer le problème du \emph{backtracking} évoqué précédemment. Dans l'exemple donné, le problème venait des règles {\iruleR{l\lor}} et {\iruleR{r\lor}}, et plus précisément du fait qu'elles ne sont pas \textbf{réversibles} :

\begin{definition}
    On dit qu'une règle $\mathsf{R}$ est \emph{réversible} dans un système $\mathbf{D}$ lorsque pour toute instance $\mathsf{r}$ de $\mathsf{R}$, si la conclusion de $\mathsf{r}$ est prouvable dans $\mathbf{D}$, alors chacune des prémisses de $\mathsf{r}$ l'est aussi.
\end{definition}

Ainsi, étant donné un séquent de la forme $\Gamma \seq A \lor B,\Delta$ prouvable dans $\mathbf{LKp}$, il n'est pas possible de déterminer si la prémisse qui nous a permis de le conclure est $\Gamma \seq A,\Delta$ ou $\Gamma \seq B,\Delta$, et il se peut que seulement l'une des deux soit prouvable dans $\mathbf{LKp}$ : on dit qu'il y a \emph{perte d'information} lors de l'application de la règle {\iruleR{l\lor}} ou {\iruleR{r\lor}}.\\

La question est donc de savoir s'il est possible de remplacer les règles irréversibles de $\mathbf{LKp}$ par des règles réversibles afin d'éliminer le besoin d'effectuer du \emph{backtracking}, ce qui nous rapprocherait de la procédure de recherche effective voulue. On peut déjà constater que de manière parfaitement symétrique, les deux règles gauches de la conjonction sont elles aussi irréversibles. Or si l'on se souvient de l'interprétation intuitive d'un séquent en termes de conjonction à gauche et de disjonction à droite, on peut être tenté de remplacer ces deux connecteurs par une simple "," de séparation des formules du séquent. De cette manière on tombe sur les deux règles d'affaiblissement, elles aussi irréversibles, ce qui suggère deux choses :

\begin{enumerate}
    \item Les règles de la conjonction et de la disjonction de $\mathbf{LKp}$ ne sont qu'une version logique des règles d'affaiblissement, permettant d'exprimer celles-ci à l'intérieur des formules. Si l'on interprète le calcul des séquents comme une théorie formelle de la relation de dérivabilité (comme cela a pu être fait dans \cite{vPN01}), c'est une façon de passer du méta-langage au langage objet.
    \item L'affaiblissement est la source de l'irréversibilité des règles, au moins dans $\mathbf{LKp}$.
\end{enumerate}

Pour appuyer la seconde suggestion, il faudrait toutefois donner une explication en termes d'affaiblissement de l'irréversibilité de {\iruleL{\lto}}, dernière règle irréversible de $\mathbf{LKp}$ que nous n'avons pas encore traitée. On peut pour cela faire appel à la notion de contextes \emph{partagés} ou \emph{indépendants} au sein d'une règle à plusieurs prémisses :

\begin{definition}
    On dit qu'une règle est \emph{à contextes partagés} lorsqu'elle est de la forme :
    \begin{displaymath}
        \prftree
            {\Gamma,\Pi_1 \seq \Lambda_1,\Delta}
            {\ldots}
            {\Gamma,\Pi_n \seq \Lambda_n,\Delta}
            {\Gamma,\Pi \seq \Lambda,\Delta}
    \end{displaymath}
    avec $n > 1$.
\end{definition}

\begin{definition}
    On dit qu'une règle est \emph{à contextes indépendants} lorsqu'elle est de la forme :
    \begin{displaymath}
        \prftree
            {\Gamma_1,\Pi_1 \seq \Lambda_1,\Delta_1}
            {\ldots}
            {\Gamma_n,\Pi_n \seq \Lambda_n,\Delta_n}
            {\Gamma_1,\ldots,\Gamma_n,\Pi \seq \Lambda,\Delta_1,\ldots,\Delta_n}
    \end{displaymath}
    avec $n > 1$.
\end{definition}

On constate alors qu'à partir du moment où une règle est à contextes indépendants, elle est irréversible. En effet, dans la présentation des règles donnée en définition, les $\Gamma_1,\ldots,\Gamma_n,\Delta_1,\ldots,\Delta_n$ peuvent correspondre à n'importe quel partitionnement du contexte du séquent conclusion si l'on se place dans le cadre d'une recherche \emph{root-first}; or seuls certains partitionnements permettent d'obtenir des prémisses prouvables, partitionnements qui ne peuvent être déterminés uniquement à partir de la conclusion.\\

\emph{A contrario}, les règles à contextes partagés {\iruleL{\lor}} et {\iruleR{\land}} sont réversibles, notamment parce que ne se pose plus la question du partitionnement du contexte, qui est simplement dupliqué dans chaque prémisse. On peut en fait remarquer que les règles à contextes indépendants se dérivent facilement des règles à contextes partagés, par simples applications itérées des règles d'affaiblissement\footnote{Cette remarque est tirée de \cite{Cur05}.} (dénotées ici par la double ligne d'inférence) :

\begin{displaymath}
    \prftree
        {\prftree[d]
            {\Gamma_1,\Pi_1 \seq \Lambda_1,\Delta_1}
            {\Gamma_1,\ldots,\Gamma_n,\Pi_1 \seq \Lambda_1,\Delta_1,\ldots,\Delta_n}}
        {\ldots}
        {\prftree[d]
            {\Gamma_n,\Pi_n \seq \Lambda_n,\Delta_n}
            {\Gamma_1,\ldots,\Gamma_n,\Pi_n \seq \Lambda_n,\Delta_1,\ldots,\Delta_n}}
        {\Gamma_1,\ldots,\Gamma_n,\Pi \seq \Lambda,\Delta_1,\ldots,\Delta_n}
\end{displaymath}

Tout cela suggère la version réversible et à contextes partagés suivante de la règle gauche de l'implication :

\begin{displaymath}
    \prftree[r]{\iruleL{\lto}}
        {\Gamma \seq A,\Delta}
        {\Gamma,B \seq \Delta}
        {\Gamma,A \lto B \seq \Delta}
\end{displaymath}

Afin de restreindre l'irréversibilité aux seules règles d'affaiblissement, il nous reste encore à trouver une version réversible des règles de la conjonction et de la disjonction. Toujours en suivant notre interprétation intuitive de la signification des séquents, on arrive aux règles suivantes :

\begin{longtabu}{>{$}c<{$}>{$}c<{$}}
    \prftree[r]{\iruleL{\land}}
        {\Gamma,A,B \seq \Delta}
        {\Gamma,A \land B \seq \Delta} &
    \prftree[r]{\iruleR{\lor}}
        {\Gamma \seq A,B,\Delta}
        {\Gamma \seq A \lor B,\Delta}
\end{longtabu}

Encore une fois on peut facilement montrer que les versions irréversibles des règles sont dérivables des versions réversibles par simple application des règles d'affaiblissement. Cela donne par exemple pour {\iruleL{l\land}} :

\begin{displaymath}
    \prftree[r]{\iruleL{\land}}
        {\prftree[r]{\iruleL{W}}
            {\Gamma,A \seq \Delta}
            {\Gamma,A,B \seq \Delta}}
        {\Gamma,A \land B \seq \Delta}
\end{displaymath}

On a donc montré que toutes les règles logiques irréversibles de $\mathbf{LKp}$ peuvent être remplacées par des versions réversibles sans perte de pouvoir déductif, puisqu'elles peuvent être dérivées de ces dernières. Réciproquement, on peut montrer que les versions réversibles sont dérivables des règles irréversibles d'origine, ce qui signifie qu'il n'y a pas non plus d'augmentation du pouvoir déductif, donc que l'on reste dans un système \textbf{équivalent} à $\mathbf{LKp}$. Pour cela, on commence par remarquer que toute règle à contextes partagés est dérivable de son équivalent à contextes indépendants, cette fois-ci en appliquant non-plus les règles d'affaiblissement, mais celles de \textbf{contraction} (dénotées comme précédemment par la double ligne d'inférence) :

\begin{displaymath}
    \prftree[d]
        {\prftree
            {\Gamma,\Pi_1 \seq \Lambda_1,\Delta}
            {\ldots}
            {\Gamma,\Pi_n \seq \Lambda_n,\Delta}
            {\Gamma,\ldots,\Gamma,\Pi \seq \Lambda,\Delta,\ldots,\Delta}}
        {\Gamma,\Pi \seq \Lambda,\Delta}
\end{displaymath}

Cela permet de montrer la dérivabilité de la règle gauche de l'implication réversible introduite précédemment. Pour les règles de la conjonction et de la disjonction, la démonstration est similaire et se base là aussi sur l'application des règles de contraction. Cela donne par exemple pour {\iruleR{\lor}} :

\begin{displaymath}
    \prftree[r]{\iruleR{C}}
        {\prftree[r]{\iruleR{r\lor}}
            {\prftree[r]{\iruleR{l\lor}}
                {\Gamma \seq A,B,\Delta}
                {\Gamma \seq A \lor B,B,\Delta}}
            {\Gamma \seq A \lor B,A \lor B,\Delta}}
        {\Gamma \seq A \lor B,\Delta}
\end{displaymath}

\chapter*{Élimination des règles structurelles}

Nous avons donc réussi à restreindre la source de l'irréversibilité de $\mathbf{LKp}$ aux seules règles d'affaiblissement, en se plaçant dans un système équivalent où toutes les règles logiques sont réversibles. Néanmoins pour cela il a fallu montrer l'interdérivabilité des versions réversibles et irréversibles des règles logiques, qui n'est possible qu'à l'aide des règles structurelles d'affaiblissement et de contraction. Or pour atteindre une procédure de recherche véritablement effective, il semble nécessaire de se débarrasser d'une part de l'affaiblissement qui est source d'irréversibilité, mais aussi de la contraction, qui en l'absence d'affaiblissement devient elle aussi source d'irréversibilité. Il suffit pour s'en convaincre de considérer l'instance de {\iruleR{C}} ayant pour conclusion $p \seq p$, dont la prémisse $p \seq p,p$ n'est prouvable qu'à l'aide d'une instance de {\iruleR{W}}. Se présentent alors deux possibilités :
\begin{enumerate}
	\item Cacher les règles structurelles à l'intérieur d'autres règles réversibles, de façon à conserver le pouvoir déductif de $\mathbf{LKp}$. Cela revient essentiellement à remplacer l'axiome d'identité par une version contextuelle $\prfbyaxiom{\irule{id}}{\Gamma,A \seq A,\Delta}$. Si l'on ajoute une règle d'introduction du connecteur 0-aire pour le faux $\bot$ et que l'on remplace la négation par sa définition $\neg A \colonequals A \lto \bot$, on obtient le système $\mathbf{G3cp}$ introduit dans \cite{vPN01}. Une stratégie de recherche effective pour $\mathbf{G3cp}$ consiste alors à simplement décomposer les formules du séquent final jusqu'à tomber sur des instances de l'axiome d'identité contextuel. Cette stratégie est déterministe (modulo l'ordre d'application des règles) car pour un séquent $\sigma$ et une formule $A$ de $\sigma$ donnés, il existe au plus une instance de règle de formule principale $A$ et de conclusion $\sigma$ qui n'est pas une instance d'axiome.
    \item Éliminer purement et simplement les règles d'affaiblissement et de contraction. On se retrouve alors dans un système de logique dite \emph{sous-structurelle}, qui possède un plus faible pouvoir déductif que $\mathbf{LKp}$. Plus précisément, si l'on retire les règles de l'implication et que l'on remplace les connecteurs classiques par leurs correspondants linéaires de manière appropriée, on se retrouve dans le fragment réversible de $\mathbf{MALL}$, un sous-ensemble du système $\mathbf{LL}$ de logique linéaire.
\end{enumerate}

Bien qu'on pourrait tout à fait se satisfaire de la première approche pour mettre en oeuvre une recherche de preuves effective en logique classique, c'est la deuxième approche qui se révèle la plus intéressante d'un point de vue théorique, et pour les fins visées dans ce mémoire. C'est en effet dans l'analyse de la dynamique qui prend place lorsque l'on optimise la recherche de preuves en logique linéaire que se dévoile le mécanisme de la \textbf{focalisation}, qui est au cœur de la ludique.

\chapter*{Des règles aux connecteurs réversibles}

Dans $\mathbf{LKp}$, la symétrie des règles de $\neg$, $\land$ et $\lor$, ainsi que la définissabilité de l'implication classique par $A \lto B \colonequals \neg A \lor B$, permettent de formuler un calcul équivalent dit \emph{monolatéral}. Le principe est de transformer les séquents bilatéraux de la forme $\Gamma \seq \Delta$ en séquents monolatéraux $\seq \neg \Gamma,\Delta$ (où $\neg \Gamma$ dénote la liste des négations des formules de $\Gamma$), afin de ne conserver que les règles droites. Les règles de la négation ayant pour fonction l'échange gauche/droite des formules à l'intérieur des séquents bilatéraux, on les supprime entièrement du calcul, ce qui nécessite de sortir la négation du langage : elle perd ainsi son statut de connecteur unaire et devient une fonction involutive sur l'ensemble des formules, définie inductivement de la manière suivante :

\begin{itemize}
    \item La négation d'un atome propositionnel \emph{positif} $p$ est l'atome propositionnel \emph{négatif} $\neg p$. On dit aussi que $p$ et $\neg p$ sont des \emph{littéraux}.
    \item La négation des formules complexes est définie à l'aide des \emph{équations} de De Morgan (\ref{eq:DM}).
        \begin{equation} \label{eq:DM}
            \neg (A \land B) = \neg A \lor \neg B \qquad \neg (A \lor B) = \neg A \land \neg B \qquad
        \end{equation}
\end{itemize}

On gagne ainsi grandement en simplicité dans la recherche de preuves, puisque le calcul contient deux fois moins de règles. Toutefois cela est possible uniquement grâce à la prouvabilité dans $\mathbf{LKp}$ des \emph{équivalences} de De Morgan, qui fait un usage crucial des règles structurelles. En effet, si l'on essaie de prouver $\neg (A \land B) \seq \neg A \lor \neg B$ dans notre système sous-structurel réversible en décomposant mécaniquement les formules jusqu'aux axiomes, on bloque de la manière suivante :

\begin{displaymath}
    \prftree[r]{\iruleL{\neg}}
        {\prftree[r]{\iruleR{\lor}}
            {\prftree[r]{\iruleR{\land}}
                {\prftree[r]{\iruleR{\neg}}
                    {\prftree[r]{\iruleR{\neg}}
                        {\prfsummary[\ding{53}]{A,B \seq A}}
                        {A \seq A,\neg B}}
                    {\seq A,\neg A,\neg B}}
                {\prftree[r]{\iruleR{\neg}}
                    {\prftree[r]{\iruleR{\neg}}
                        {\prfsummary[\ding{53}]{B,A \seq B}}
                        {B \seq B,\neg A}}
                    {\seq B,\neg A,\neg B}}
                {\seq A \land B,\neg A,\neg B}}
            {\seq A \land B,\neg A \lor \neg B}}
        {\neg (A \land B) \seq \neg A \lor \neg B}
\end{displaymath}

On voit que la règle à contextes partagés {\iruleR{\land}} nous oblige à distribuer le contexte $\neg A,\neg B$ dans les deux prémisses, ce qui fait qu'en l'absence d'affaiblissement il est impossible d'arriver à des instances d'axiome.\\

Pour retrouver la symétrie du calcul en l'absence des règles structurelles, il est nécessaire de décomposer la conjonction et la disjonction en deux variantes dites \emph{multiplicatives} et \emph{additives} (cf. Tab. \ref{tab:conn_mall}), qui correspondent respectivement aux règles à contextes indépendants et partagés.

\begin{table}[h]
\begin{longtabu}{|>{\bf}c|c|c|}
    \hline
    \rowfont{\bfseries} & Multiplicative & Additive \\
    \hline
    Conjonction & $\otimes$ (\emph{fois}) & $\with$ (\emph{avec}) \\
    \hline
    Disjonction & $\parr$ (\emph{par}) & $\oplus$ (\emph{plus}) \\
    \hline
\end{longtabu}
\caption{Connecteurs de $\mathbf{MALL}$}
\label{tab:conn_mall}
\end{table}

On peut maintenant reformuler les règles logiques de notre système sous-structurel réversible dans ce nouveau langage, en faisant comme mentionné tout à l'heure exception de l'implication\footnote{Ce choix est justifié d'une part par la difficulté à définir de façon symétrique et au sein d'un même système une variante multiplicative et additive de l'implication (voir \cite{OP99} pour un traitement de la question à l'aide de la logique sous-structurelle $\mathbf{BI}$); et d'autre part car cela n'est pas utile au développement du mécanisme de focalisation.}\footnote{On reprend ici la notation de la négation à l'aide du symbole $\bot$ en exposant des formules introduite par Girard, car elle permet de faire le lien entre la négation de la logique linéaire et la notion d'\emph{orthogonalité} centrale en ludique.} :

\begin{longtabu}{>{$}c<{$}>{$}c<{$}}
    \prftree[r]{\iruleL{\cdot^\bot}}
        {\Gamma \seq A,\Delta}
        {\Gamma,A^\bot \seq \Delta} &
    \prftree[r]{\iruleR{\cdot^\bot}}
        {\Gamma,A \seq \Delta}
        {\Gamma \seq A^\bot,\Delta} \\

    \prftree[r]{\iruleL{\otimes}}
        {\Gamma,A,B \seq \Delta}
        {\Gamma,A \otimes B \seq \Delta} &
    \prftree[r]{\iruleR{\with}}
        {\Gamma \seq A,\Delta}
        {\Gamma \seq B,\Delta}
        {\Gamma \seq A \with B,\Delta} \\

    \prftree[r]{\iruleL{\oplus}}
        {\Gamma,A \seq \Delta}
        {\Gamma,B \seq \Delta}
        {\Gamma,A \oplus B \seq \Delta} &
    \prftree[r]{\iruleR{\parr}}
        {\Gamma \seq A,B,\Delta}
        {\Gamma \seq A \parr B,\Delta}
\end{longtabu}

Afin de pouvoir formuler un calcul monolatéral équivalent, il nous reste encore à montrer que les équivalences de De Morgan multiplicatives (\ref{eq:mul_DM}) et additives (\ref{eq:add_DM}) sont prouvables dans le système. Or il se trouve qu'elles ne le sont pas, ce qui est dû au fait que nous nous sommes limités, dans notre quête d'un algorithme de recherche de preuves effectif, aux règles réversibles. Par exemple si l'on veut prouver $(A \otimes B)^\bot \seq A^\bot \parr B^\bot$, on est obligé de décomposer $A \otimes B$ à droite du séquent, ce qui n'est pas possible sans la règle {\iruleR{\otimes}}.
\begin{align}
    (A \otimes B)^\bot \equiv A^\bot \parr B^\bot &\qquad (A \parr B)^\bot \equiv A^\bot \otimes B^\bot \label{eq:mul_DM} \\
    (A \with B)^\bot \equiv A^\bot \oplus B^\bot &\qquad (A \oplus B)^\bot \equiv A^\bot \with B^\bot \label{eq:add_DM}
\end{align}
Dans l'objectif de retrouver les équivalences de De Morgan (ainsi qu'un peu de pouvoir déductif), nous allons donc rajouter les règles symétriques et irréversibles manquantes, ce qui nous place très exactement dans le calcul bilatéral de $\mathbf{MALL}$ :

\begin{center}
    \begin{tabu}{>{$}c<{$}>{$}c<{$}}
    \prftree[r]{\iruleL{l\with}}
        {\Gamma,A \seq \Delta}
        {\Gamma,A \with B \seq \Delta} &
    \multirow{2}{*}{
    \prftree[r]{\iruleR{\otimes}}
        {\Gamma \seq A,\Delta}
        {\Gamma' \seq B,\Delta'}
        {\Gamma,\Gamma' \seq A \otimes B,\Delta,\Delta'}} \\
    \prftree[r]{\iruleL{r\with}}
        {\Gamma,B \seq \Delta}
        {\Gamma,A \with B \seq \Delta} & \\

    \multirow{2}{*}{
    \prftree[r]{\iruleL{\parr}}
        {\Gamma,A \seq \Delta}
        {\Gamma',B \seq \Delta'}
        {\Gamma,\Gamma',A \parr B \seq \Delta,\Delta'}} &
    \prftree[r]{\iruleR{l\oplus}}
        {\Gamma \seq A,\Delta}
        {\Gamma \seq A \oplus B,\Delta} \\ &
    \prftree[r]{\iruleR{r\oplus}}
        {\Gamma \seq B,\Delta}
        {\Gamma \seq A \oplus B,\Delta}
\end{tabu}
\end{center}

À partir de là il est facile de formuler le calcul monolatéral de $\mathbf{MALL}$, en procédant de la même manière que pour $\mathbf{LKp}$. Chaque connecteur ne possédant plus qu'une règle qui lui est associée\footnote{Deux pour $\oplus$, mais qui possèdent les mêmes propriétés.}, on peut alors parler de \emph{connecteurs (ir)réversibles}, ce qui va s'avérer très important pour définir la focalisation. Ainsi $\otimes$ et $\oplus$ sont-ils irréversibles, tandis que $\with$ et $\parr$ sont réversibles. On parle aussi parfois de connecteurs \textbf{positifs} et \textbf{négatifs}, en relation à la notion de \emph{polarité} qui étend la réversibilité aux formules\footnote{On voit ici la justification du choix des symboles, les connecteurs de même polarité étant la version retournée l'un de l'autre. On notera toutefois que cette justification n'est venue qu'\textit{a posteriori}, l'étude de la polarisation ayant eu lieu après la découverte de la logique linéaire.} :

\begin{definition}[Polarité]
	On dit qu'une formule $A$ est \emph{positive} lorsque $A$ est un atome propositionnel positif, ou lorsque le connecteur principal de $A$ est irréversible.
	On dit qu'une formule $A$ est \emph{négative} lorsque $A$ est un atome propositionnel négatif, ou lorsque le connecteur principal de $A$ est réversible.
\end{definition}

\chapter*{La focalisation}

En réintroduisant des règles irréversibles pour retrouver la symétrie de la logique classique sans les règles structurelles, on a aussi réintroduit la nécessité d'effectuer du \textit{backtracking} lors de la recherche de preuves. Il semble donc que ce que nous avons gagné en nombre de règles et en pouvoir déductif soit perdu en complexité de la procédure de recherche. Or il se trouve que grâce à la polarisation des formules, il est possible de restreindre l'espace de recherche à des preuves dites \emph{focalisées}. L'idée est d'organiser la recherche en une alternance de deux phases :
\begin{itemize}
	\item Une phase \emph{asynchrone} entièrement déterministe où l'on décompose les formules négatives dans n'importe quel ordre jusqu'à n'avoir que des littéraux. Cette phase est toujours exécutée en priorité, ce qui signifie que dès que des formules négatives non-atomiques apparaissent dans un séquent, elle doivent être décomposées.
	\item Une phase \emph{synchrone} non-déterministe où l'on décompose les formules positives, ce qui nécessite donc de faire du \textit{backtracking}. De plus, une fois qu'une formule positive est sélectionnée pour la décomposition, on se \emph{focalise} dessus, ce qui signifie que l'on continue à décomposer ses sous-formules jusqu'à tomber sur des formules négatives ou des atomes positifs. Cela permet de réduire grandement le non-déterminisme lié au processus de sélection de la formule à décomposer, qui par exemple était responsable de la redondance des preuves dans $\mathbf{G3cp}$ ne différant que dans l'ordre d'application des règles.
\end{itemize}
Prenons par exemple le début de preuve suivant, dans lequel on souligne pour chaque instance de règle la formule principale :
\begin{displaymath}
	\prftree[r]{\iruleR{\otimes}}
		{\prfsummary{\seq A,F}}
		{\prftree[r]{\iruleR{\oplus}}
			{\prftree[r]{\iruleR{\oplus}}
				{\prfsummary{\seq B,D}}
				{\prfsummary{\seq C,G}}
				{\seq \underline{B \otimes C},D,G}}
			{\seq B \otimes C,\underline{D \oplus E},G}}
		{\seq \underline{A \otimes (B \otimes C)},D \oplus E,F,G}
\end{displaymath}
Cette preuve n'est pas focalisée, car une fois la formule $A \otimes (B \otimes C)$ sélectionnée, il est nécessaire de continuer à décomposer ses sous-formules positives; or à l'étape suivante ce n'est pas $B \otimes C$ qui est sélectionnée, mais $D \oplus E$, qui même si elle est positive n'est pas sous-formule de $A \otimes (B \otimes C)$. En corrigeant cela, on obtient la preuve :
\begin{displaymath}
	\prftree[r]{\iruleR{\otimes}}
		{\prfsummary{\seq A,F}}
		{\prftree[r]{\iruleR{\otimes}}
			{\prftree[r]{\iruleR{\oplus}}
				{\prfsummary{\seq B,D}}
				{\seq B,\underline{D \oplus E}}}
			{\prfsummary{\seq C,G}}
			{\seq \underline{B \otimes C},D \oplus E,G}}
	{\seq \underline{A \otimes (B \otimes C)},D \oplus E,F,G}
\end{displaymath}
Cette preuve est focalisée à condition que les formules $F$ et $G$ ne soit pas négatives et non-atomiques, auquel cas selon la condition de priorité de la phase asynchrone elles auraient dûes être décomposées en premier.\\

Dans son article \cite{And92}, Andreoli montre que l'ensemble des preuves focalisées formalisées dans son système $\bm{\Sigma_3}$ est un sous-ensemble complet des preuves de $\mathbf{LL}$, c'est-à-dire qu'il existe une preuve focalisée pour chaque formule dérivable dans $\mathbf{LL}$. Le théorème s'applique facilement à $\mathbf{MALL}$ qui est un sous-système de $\mathbf{LL}$, ce qui signifie que l'on peut effectivement quotienter l'espace de recherche par ces preuves focalisées. Il est intéressant de remarquer qu'Andreoli arrive à ce résultat de manière parfaitement constructive, de sorte qu'il est en principe possible d'extraire de la démonstration qu'il donne une procédure mécanique permettant de transformer n'importe quelle preuve de $\mathbf{LL}$ en une preuve de $\bm{\Sigma_3}$. On peut ici faire le parallèle avec le \textit{Haupstatz}\footnote{Qui par ailleurs reste valide dans $\mathbf{MALL}$.} évoqué plus tôt, qui de manière similaire permet de restreindre l'espace des preuves via la procédure d'\emph{élimination des coupures}. On comprend alors mieux l'intérêt porté par Girard sur ce mécanisme de focalisation, compte-tenu de la place centrale qu'il accordait déjà au $Haupstatz$ dans ses travaux en théorie de la démonstration, au point de rejeter tout système de calcul des séquents ne vérifiant pas ce théorème.\\

On a donc vu que les preuves focalisées consistent en une succession de "blocs" d'applications de règles réversibles lors de la phase asynchrone, et irréversibles lors de la phase synchrone. On reconnaît ici la forme d'un dialogue entre deux processus qui interagiraient chacun leur tour dans une sorte de jeu démonstratif, rejoignant les interprétations ludiques de la logique tels que la logique dialogique de Lorenzen ou la sémantique des jeux GTS de Hintikka. Toutefois pour rendre cette correspondance plus claire et plus précise, il serait commode de pouvoir réduire ces blocs de règles multiples en instances d'une unique règle associée à chaque phase/processus/joueur. Or pour réduire notre système à deux règles, une positive pour $\otimes,\oplus$ et une négative pour $\with,\parr$, il est d'abord nécessaire de trouver deux formes "normales" des formules de $\mathbf{MALL}$ auxquelles toute formule de l'une ou l'autre polarité peut-être ramenée. Pour cela on peut faire appel à la distributivité des connecteurs multiplicatifs sur les additifs :
\begin{align}
A \otimes (B \oplus C) \equiv (A \otimes B) \oplus (A \otimes C) \label{eq:dist_pos} \\
A \parr (B \with C) \equiv (A \parr B) \with (A \parr C) \label{eq:dist_neg}
\end{align}
Ainsi toute formule positive (resp. négative) peut être exprimée de manière équivalente dans $\mathbf{MALL}$ comme une formule de la forme $\bigoplus_{i \in I}\bigotimes_{j \in J_i}A_{i,j}$ (resp. $\bigwith_{i \in I}\bigparr_{j \in J_i}A_{i,j}$)\footnote{Notons qu'il est nécessaire pour que cette réduction soit valide de vérifier la commutativité des connecteurs, ainsi que leur associativité qui permet d'utiliser la formulation $n$-aire.}. On parle aussi parfois de \emph{connecteur synthétique} positif (resp. négatif) entre les formules $A_{i,j}$.\\

Essayons maintenant de généraliser les blocs de règles évoqués précédemment à l'aide des connecteurs synthétiques. Tout d'abord pour la phase asynchrone, on peut partir du principe que toutes les formules négatives du séquent à prouver ont été regroupées à l'aide d'un $\parr$ $n$-aire. On se retrouve donc avec un séquent ne contenant qu'une seule formule négative en forme normale, ainsi qu'un contexte $\Delta$ de formules positives. La phase asynchrone étant déterministe, la décomposition du séquent prendra systématiquement la forme suivante\footnote{On se permet ici une formulation $n$-aire des règles de $\mathbf{MALL}$, qui se déduit assez directement de leur formulation binaire.} :
\begin{displaymath}
	\prftree[r]{\iruleR{\with}}
		{\prftree[r]{\iruleR{\parr}}
			{\prfsummary{\seq A_{1,1},\ldots,A_{1,n_1},\Delta}}
			{\seq \bigparr_{1 \leqslant j \leqslant n_1} A_{1,j},\Delta}}
		{\ldots}
		{\prftree[r]{\iruleR{\parr}}
			{\prfsummary{\seq A_{k,1},\ldots,A_{k,n_k},\Delta}}
			{\seq \bigparr_{1 \leqslant j \leqslant n_k} A_{k,j},\Delta}}
		{\seq \bigwith_{1 \leqslant i \leqslant k} \bigparr_{1 \leqslant j \leqslant n_i} A_{i,j},\Delta}
\end{displaymath}
Ce qui revient à donner une dérivation de la règle négative :
\begin{displaymath}
	\prftree[r]{\irule{\with\parr}}
		{\seq A_{1,1},\ldots,A_{1,n_1},\Delta}
		{\ldots}
		{\seq A_{k,1},\ldots,A_{k,n_k},\Delta}
		{\seq \bigwith_{1 \leqslant i \leqslant k} \bigparr_{1 \leqslant j \leqslant n_i} A_{i,j},\Delta}
\end{displaymath}

Pour la phase synchrone, on peut assumer que toute les formules négatives non-atomiques ont déjà été décomposées lors de la phase asynchrone. On se retrouve donc avec un séquent contenant une formule positive non-atomique en forme normale sur laquelle on va se focaliser, ainsi qu'un contexte $\Delta$ de formules positives ou atomiques. La phase synchrone étant non-déterministe, il ne va pas être possible de formuler une unique règle comme pour la phase asynchrone. En effet, la décomposition du séquent prendra typiquement la forme suivante :
\begin{displaymath}
	\prftree[r]{\iruleR{\oplus_p \quad (\text{où } 1 \leqslant p \leqslant k)}}
		{\prftree[r]{\iruleR{\otimes \quad (\text{où les } \Delta_i \text{ forment une partition de } \Delta)}}
			{\prfsummary{\seq A_{p,1},\Delta_1}}
			{\ldots}
			{\prfsummary{\seq A_{p,n_p},\Delta_{n_p}}}
			{\seq \bigotimes_{1 \leqslant j \leqslant n_p} A_{p,j},\Delta}}
		{\seq \bigoplus_{1 \leqslant i \leqslant k} \bigotimes_{1 \leqslant j \leqslant n_i} A_{i,j},\Delta}
\end{displaymath}
Ce qui revient à donner une dérivation du schéma de règles positives :
\begin{displaymath}
	\prftree[r]{\irule{\oplus\otimes_p}}
		{\seq A_{p,1},\Delta_1}
		{\ldots}
		{\seq A_{p,n_p},\Delta_{n_p}}
	{\seq \bigoplus_{1 \leqslant i \leqslant k} \bigotimes_{1 \leqslant j \leqslant n_i} A_{i,j},\Delta}
\end{displaymath}

Maintenant si l'on s'autorise à revenir à un système de séquents bilatéraux tout en conservant la définition de la négation comme fonction involutive sur l'ensemble des formules, on peut obtenir les schémas de dérivations suivants :
\begin{displaymath}
	\prftree[r]{\iruleL{\cdot^\bot}}
		{\prftree[r]{\irule{\with\parr}}
			{\seq A_{1,1},\ldots,A_{1,n_1},\Delta}
			{\ldots}
			{\seq A_{k,1},\ldots,A_{k,n_k},\Delta}
			{\seq \bigwith_{1 \leqslant i \leqslant k} \bigparr_{1 \leqslant j \leqslant n_i} A_{i,j},\Delta}}
		{\bigoplus_{1 \leqslant i \leqslant k} \bigotimes_{1 \leqslant j \leqslant n_i} A_{i,j}^\bot \seq \Delta}
\end{displaymath}
\begin{displaymath}
	\prftree[r]{\irule{\oplus\otimes_p}}
		{\prftree[r]{\iruleR{\cdot^\bot}}
			{A_{p,1} \seq \Delta_1}
			{\seq A_{p,1}^\bot,\Delta_1}}
		{\ldots}
		{\prftree[r]{\iruleR{\cdot^\bot}}
			{A_{p,n_p} \seq \Delta_{n_p}}
			{\seq A_{p,n_p}^\bot,\Delta_{n_p}}}
		{\seq \bigoplus_{1 \leqslant i \leqslant k} \bigotimes_{1 \leqslant j \leqslant n_i} A_{i,j}^\bot,\Delta}
\end{displaymath}
Cela nous permet de donner une formulation bilatérale des règles positives et négative, dans laquelle les formules décomposées (ou introduites en lecture \textit{top-down}) sont toujours en forme normale positive :
\begin{longtabu}{>{$}c<{$}>{$}c<{$}}
	\prftree[r]{\iruleL{\oplus\otimes}}
		{\seq A_{1,1},\ldots,A_{1,n_1},\Delta}
		{\ldots}
		{\seq A_{k,1},\ldots,A_{k,n_k},\Delta}
		{\bigoplus_{1 \leqslant i \leqslant k} \bigotimes_{1 \leqslant j \leqslant n_i} A_{i,j}^\bot \seq \Delta} &
	\prftree[r]{\iruleR{\oplus\otimes_p}}
		{A_{p,1} \seq \Delta_1}
		{\ldots}
		{A_{p,n_p} \seq \Delta_1}
		{\seq \bigoplus_{1 \leqslant i \leqslant k} \bigotimes_{1 \leqslant j \leqslant n_i} A_{i,j}^\bot,\Delta}
\end{longtabu}
On peut constater que dans le système comprenant uniquement l'axiome d'identité (en version atomique et bilatérale) ainsi que ces deux règles\footnote{Système que l'on appellera dorénavant $\mathbf{HS}$, pour "calcul HyperSéquentialisé de $\mathbf{MALL}$".}, tout séquent apparaissant dans une preuve contient au plus une formule à gauche et un nombre fini de formules à droite, et toutes les formules sont positives. On peut en fait associer la zone gauche (resp. droite) de ces séquents à la zone à droite (resp. à gauche) du symbole $\Downarrow$ dans les séquents du système $\bm{\Sigma_3}$ de \cite{And92}, la focalisation correspondant alors au passage à gauche des formules via la règle positive {\iruleR{\oplus\otimes_p}}.

\chapter*{La ludique}

En éliminant les règles structurelles d'affaiblissement et de contraction et en se restreignant à un espace de preuves sans coupures, focalisées et à base de connecteurs synthétiques, nous avons réussi à établir un système logique $\mathbf{HS}$ comprenant uniquement un axiome d'identité et deux règles d'inférence, tout en gardant un compromis intéressant entre pouvoir expressif et tractabilité de la procédure de recherche de preuves. En réalité le qualificatif de règle d'"inférence" n'est plus vraiment adapté à la problématique à laquelle répond le système, qui s'est définitivement éloigné de l'usage déductif qui avait encore sa place dans un système tel que $\mathbf{LKp}$ pour adopter pleinement la lecture \textit{bottom-up} de la recherche ou \emph{tentative} de preuve.\\

On peut en fait voir la ludique comme une lecture littérale de ce paradigme, où l'accent est mis sur le mot "tentative", qui implique la possibilité d'un \emph{échec} traduisant l'invalidité de la preuve en train d'être construite pour le séquent considéré. On retrouvait déjà cette notion d'échec dans le mécanisme du \textit{backtracking}, qui ne s'active que lorsque un séquent conclusion d'aucune règle valide est atteint. Le problème du \textit{backtracking} est qu'il se situe à un niveau métalogique, celui de la procédure de recherche, et qu'il n'est donc pas représenté au sein même du système. Un des actes fondateurs de la ludique va être d'officialiser la possibilité de l'échec, en remplaçant l'axiome d'identité par un axiome généralisé permettant d'introduire n'importe quel séquent, la règle du \textbf{daimon} :

\begin{displaymath}
	\prftree[r]{\irule{\dai}}
		{\seq \Lambda}
\end{displaymath}

Bien sûr cette modification seule ne ferait que rendre le système inconsistant en autorisant la construction de preuves pour $A$ et $\neg A$, ce qui n'est pas l'objectif visé. En fait cette généralisation de l'axiome implique aussi une généralisation de la notion de preuve, qui passe par l'oubli de la notion de \emph{formule} : ainsi $\Lambda$ n'est-il pas une suite finie de formules, mais un ensemble de \textbf{loci}, c'est-à-dire l'ensemble des lieux où seraient normalement inscrites les formules. Puisque les preuves de $\mathbf{HS}$ sont sans coupures, elles satisfont la propriété de la sous-formule, ce qui se traduit au niveau des \textit{loci} par le fait qu'un \textit{locus} doit pouvoir s'exprimer uniquement à partir des \textit{loci} situés plus bas que lui dans la preuve. Mieux, un \textit{locus} doit correspondre à la branche de l'arbre de preuve qui suit la décomposition de la conclusion/racine jusqu'à la sous-formule associée au \textit{locus}. Toutes ces propriétés peuvent être capturées dans les définitions suivantes :
\begin{definition}
	Un \emph{biais} est un nombre entier naturel, dénoté par une lettre latine minuscule ($i,j,k,\ldots$). Une \emph{ramification} est un ensemble fini de biais, dénoté par une lettre latine majuscule ($I,J,K,\ldots$). Un \emph{locus}, aussi appelé \emph{adresse}, est une suite finie $\langle i_1,\ldots,i_n \rangle$ de biais, dénotée par une lettre grecque minuscule ($\sigma,\tau,\nu,\xi,\ldots$).
\end{definition}
On dispose aussi d'un opérateur de concaténation $\ast$ pour les \textit{loci}, accompagné de quelques raccourcis de notation :
\begin{itemize}
	\item $\sigma \ast i \colonequals \sigma \ast \langle i \rangle$
	\item $\sigma i \colonequals \sigma \ast i$
	\item $\sigma \ast I \colonequals \{\sigma \ast i \mid i \in I \}$
\end{itemize}

La ludique va reprendre les séquents contenant au plus une formule à gauche de $\mathbf{HS}$, maintenant appelés \textbf{fourchettes} :
\begin{definition}
	Une \emph{fourchette} est une expression $\Xi \seq \Lambda$, où :
	\begin{enumerate}
		\item $\Xi$ et $\Lambda$ sont des ensembles finis de \textit{loci} deux à deux disjoints.
		\item $\Xi$ contient au plus un \textit{locus}, appelé le \emph{manche}, les \text{loci} de $\Lambda$ étant les \emph{dents}.
	\end{enumerate}
\end{definition}
Les preuves généralisées de la ludique, appelées \textbf{dessins}, sont des arbres de hauteur potentiellement infinie dont les noeuds sont des fourchettes, et que l'on construit à l'aide du \textit{daimon} ainsi que d'une version adaptée et légèrement modifiée des règles de $\mathbf{HS}$ :\\

\begin{longtabu}{p{17em}p{17em}}
	\rowfont{\centering\bfseries}
	Règle positive\vspace{3mm} &
	Règle négative\vspace{3mm} \\
	\rowfont{\centering}
	$\prftree[r]{\irule{\xi,I}}
		{\ldots}
		{\xi \ast i \seq \Lambda_i}
		{\ldots}
		{\seq \xi,\Lambda}$ &
	$\prftree[r]{\irule{\xi,\mathcal{N}}}
		{\ldots}
		{\seq \xi \ast I,\Lambda_I}
		{\ldots}
		{\xi \seq \Lambda}$ \\
	\rowfont{\footnotesize}
	$I$ est une ramification telle que la règle comporte une prémisse pour chaque $i \in I$, et les $\Lambda_i$ sont deux à deux disjoints et inclus dans $\Lambda$. &
	$\mathcal{N}$ est un ensemble possiblement infini de ramifications, le \emph{répertoire} de la règle, tel que celle-ci comporte une prémisse pour chaque $I \in \mathcal{N}$, et les $\Lambda_I$ sont inclus dans $\Lambda$.
\end{longtabu}

On a donc repris les règles positives et négative de $\mathbf{HS}$ mais dans une forme plus épurée, où l'on oublie toute l'information relative aux formules pour ne retenir que la relation d'ordre qu'elles entretiennent, traduite ici dans la notion de \emph{sublocus} :
\begin{definition}
	On dit que $\tau$ est un \emph{sublocus} de $\sigma$ lorsqu'il existe un $\nu$ tel que $\tau = \sigma \ast \nu$.
\end{definition}

Les dessins et les preuves de $\mathbf{HS}$ se distinguent ainsi crucialement sur deux aspects :
\begin{enumerate}
	\item Les dessins peuvent s'étendent de manière infinie, non seulement en hauteur du fait qu'il n'est nullement obligé de donner des feuilles aux branches sous la forme d'instances du \textit{daimon}, mais aussi en largeur via le nombre potentiellement infini de prémisses de la règles négative. Ceci n'est pas le cas pour les preuves traditionnels du calcul des séquents (et en particulier celle de $\mathbf{HS}$), dont les branches se terminent obligatoirement par des instances d'axiomes, et dont les règles ne contiennent dans la plupart des cas qu'un nombre fini de prémisses.
	\item La gestion du contexte est moins restrictive dans les dessins : dans la règle positive les $\Lambda_i$ n'ont plus besoin de former une partition totale de $\Lambda$, et dans la règle négative les $\Lambda_I$ ne sont pas nécessairement des recopies complètes de $\Lambda$. Cela revient en fait à admettre une forme d'affaiblissement, autorisant des instances de règles où une partie du contexte disparaît telles que :\\
	\begin{longtabu}{>{$}c<{$}>{$}c<{$}}
		\prftree[r]{\irule{\xi,\{1,2\}}}
		{\xi \ast 1 \seq \alpha}
		{\xi \ast 2 \seq \beta}
		{\seq \xi,\alpha,\beta,\gamma} &
		\prftree[r]{\irule{\xi,\{\{1,2\},\{1,3\}\}}}
		{\seq \xi \ast \{1,2\},\alpha,\beta}
		{\seq \xi \ast \{1,3\},\alpha}
		{\xi \seq \alpha,\beta,\gamma}
	\end{longtabu}
\end{enumerate}

La ludique oublie donc complètement la problématique de l'efficacité lors de la recherche de preuve, d'une part en autorisant des dessins aux dimensions infinies, ce qui correspondrait à une recherche qui ne termine jamais; et d'autre part en admettant une forme d'affaiblissement, rendant toutes les règles irréversibles. En réalité, les notions mêmes de recherche de preuve et de réversibilité n'ont plus de sens dans le cadre ludique, un dessin pris dans son individualité ne pouvant être considéré comme une \emph{preuve} : il faut plutôt le voir comme une \emph{stratégie de dialogue} qui attend d'être testée par d'autres stratégies, test qui a lieu lorsque deux agents exécutent leurs stratégies respectives lors d'une \emph{interaction dialogique}. Si l'interaction se déroule correctement, c'est-à-dire si les deux agents atteignent un consensus (représenté par la règle du \textit{daimon}), alors on dit que l'interaction \emph{converge}. Sinon se créé un dissensus entre les deux agents, et on dit que l'interaction \emph{diverge}. Ce processus d'interaction entre dessins correspond au processus d'élimination des coupures entre preuves en calcul des séquents, et peut être formalisé via la notion de \emph{normalisation d'un réseau de coupure}. Toutefois la présentation de cette notion complexe dépasse notre cadre d'introduction à la ludique et à ses objets de base que sont les dessins.\\

Pour finir, il peut être intéressant de donner un exemple de modélisation en ludique d'une situation concrète de dialogue, afin d'illustrer l'interprétation pragmatique des dessins donnée ci-dessus.

% TODO: exemple de modélisation d'une interaction dialogique

\chapter*{Conclusion}

% TODO: présentation formelle de la notion de coupure comme co-présence en un même lieu, puis du processus de normalisation/élimination des coupures

% TODO: notion de dessein comme véritables objets de la ludique

% TODO: reconstruction interactive des types/formules via la notion de comportement

En se concentrant sur la conception d'un système le plus expressif possible admettant un maximum de dessins, tout en restant suffisemment consistant d'un point de vue calculatoire pour pouvoir reconstruire les règles logiques. On peut ici faire un parallèle avec le $\lambda$-calcul dans ses versions pur et typée, où le $\lambda$-calcul pur serait associé à la ludique dans son oubli de l'information de typage, et le $\lambda$-calcul typé au système $\mathbf{HS}$\footnote{On peut rendre ce parallèle entre types et formules plus précis via la correspondance de Curry-Howard.}. Toutefois en $\lambda$-calcul le typage se fait de manière "\textit{ad hoc}" en attribuant à chaque variable un type atomique, tandis que l'ambition de la ludique est d'expliquer la notion de formule/type comme un produit de la dynamique calculatoire, comprise comme l'interaction entre les dessins/$\lambda$-termes.

\nocite{Tai68}
\nocite{Lec11}
\nocite{Gir03}
\nocite{Gir06}

\bibliographystyle{amsalpha}
\bibliography{main}

\end{document}
